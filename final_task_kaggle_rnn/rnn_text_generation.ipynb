{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "064c5e42",
   "metadata": {},
   "source": [
    "# Finale Aufgabe für Praktikum Deep Learning <br>Textgenerierung mit RNN: Textgenerierung\n",
    "\n",
    "* **Name:** Fabian Schotte\n",
    "* **Email:** fabian.schotte@rwu.de\n",
    "* **Matrikelnummer:** 35604\n",
    "* **Studiengang:** Angewandte Informatik"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e9033a0",
   "metadata": {},
   "source": [
    "## 1. GRU-Modell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d132560e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-09 17:45:07.207427: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-06-09 17:45:07.214528: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1749491107.222845  155976 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1749491107.225289  155976 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-06-09 17:45:07.234002: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "I0000 00:00:1749491108.424153  155976 gpu_process_state.cc:201] Using CUDA malloc Async allocator for GPU: 0\n",
      "I0000 00:00:1749491108.425143  155976 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 1098 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4070, pci bus id: 0000:01:00.0, compute capability: 8.9\n",
      "I0000 00:00:1749491113.691700  156114 cuda_dnn.cc:529] Loaded cuDNN version 90501\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hey, \n",
      "Just...  Goodness! ( I see no cheeses\n",
      "Gothangs broke  I have to work at 4am now  #canucks!  I guess perfect sanacious and no where don`t you say hi!!! It would! Too much lol\n",
      "Think I have sunshine for _yang 'happy T-Laughs economic wtl! I`m CRick in college in the teenale! I wish i was even more watching the outsiders  smelly it says it new someone   i need for absolutly not manicure\n",
      "WELc like christmas tear, I`m so freaking both of your tweets  I was a wonderful move to my bfast with me. I better get to see\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.environ['TF_GPU_ALLOCATOR'] = 'cuda_malloc_async'\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gc\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras import backend as K\n",
    "\n",
    "# 1. Load your trained model\n",
    "model = keras.models.load_model('work/models/gru_model_1.keras')\n",
    "\n",
    "# 2. Rebuild your char‐level vocabulary/lookups from the training CSV\n",
    "df = pd.read_csv('work/kaggle_sentiment/tweet_sentiment_train.csv',\n",
    "                 encoding='utf-8', encoding_errors='replace')\n",
    "# concatenate all tweets into one long string\n",
    "text = df['text'].str.cat(sep='\\n')\n",
    "\n",
    "# get sorted list of all unique characters in the corpus\n",
    "vocab = sorted(set(text))\n",
    "vocab_size = len(vocab) + 1   # +1 for the OOV token that StringLookup will insert\n",
    "\n",
    "# make lookup layers exactly as in your notebook\n",
    "ids_from_chars = keras.layers.StringLookup(\n",
    "    vocabulary=vocab, mask_token=None  # no PAD token\n",
    ")\n",
    "chars_from_ids = keras.layers.StringLookup(\n",
    "    vocabulary=ids_from_chars.get_vocabulary(), \n",
    "    invert=True, \n",
    "    mask_token=None\n",
    ")\n",
    "\n",
    "# helper to turn a string into a vector of IDs\n",
    "def text_to_ids(s: str):\n",
    "    # unicode_split → a TF string Tensor of shape (len(s),)\n",
    "    chars = tf.strings.unicode_split([s], 'UTF-8')\n",
    "    return ids_from_chars(chars)\n",
    "\n",
    "# helper to turn a list of IDs back into text\n",
    "def ids_to_text(ids):\n",
    "    return tf.strings.reduce_join(chars_from_ids(ids), axis=-1).numpy().astype(str)\n",
    "\n",
    "# 3. Seed + pad to the fixed window length your model expects\n",
    "seed = \"Hey, \"\n",
    "seq_length = 100   # <— use whatever you trained with\n",
    "\n",
    "# convert seed to IDs and pad or trim to length\n",
    "seed_ids = text_to_ids(seed).numpy()[0]   # shape (len(seed),)\n",
    "seed_ids = seed_ids[-seq_length:]        # keep last seq_length chars\n",
    "seed_ids = np.expand_dims(seed_ids, 0)    # make batch of 1\n",
    "seed_ids = keras.preprocessing.sequence.pad_sequences(\n",
    "    seed_ids, maxlen=seq_length, padding='pre'\n",
    ")\n",
    "\n",
    "# 4. Generate one char at a time\n",
    "generated_ids = []\n",
    "num_chars = 512\n",
    "temperature = 1.0\n",
    "\n",
    "for _ in range(num_chars):\n",
    "    # predict next‐char distribution\n",
    "    preds = model.predict(seed_ids, verbose=0)[0, -1, :]  # (vocab_size,)\n",
    "    # apply temperature\n",
    "    preds = np.log(preds + 1e-8) / temperature\n",
    "    preds = np.exp(preds) / np.sum(np.exp(preds))\n",
    "    # sample\n",
    "    next_id = np.random.choice(len(preds), p=preds)\n",
    "    generated_ids.append(next_id)\n",
    "    # shift window and append\n",
    "    seed_ids = np.roll(seed_ids, -1, axis=1)\n",
    "    seed_ids[0, -1] = next_id\n",
    "\n",
    "# 5. Decode back to a string\n",
    "gen_text = ids_to_text(np.array([generated_ids]))\n",
    "print(seed)\n",
    "\n",
    "for text in gen_text:\n",
    "    print(text)\n",
    "\n",
    "del model\n",
    "K.clear_session()\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b45c207",
   "metadata": {},
   "source": [
    "## 2. GRU-Modell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "515c2367",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hey, \n",
      "overheat nooooo where do u stay awaynt look\n",
      "should be the non-logies duble\n",
      "_clorahley Don`t worry you didn`t, I like them\n",
      " you are the tweet working\n",
      "I want to go to work ways. Just too late for breakfast after a little phan`s first stalk tomorrow!!!\n",
      " 'you`re the only one out! I want to write back then!\n",
      " Poor LostMa 4. Ha. tnagged it on the glob on the ensopro\n",
      " As  ? http://blip.fm/~5yyt3\n",
      "It`s gonna be a dear and actione of my face after abit;\n",
      "I don`t want to be home til since the world\n",
      " Thanks, I got some n\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.environ['TF_GPU_ALLOCATOR'] = 'cuda_malloc_async'\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gc\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras import backend as K\n",
    "\n",
    "# 1. Load your trained model\n",
    "model = keras.models.load_model('work/models/gru_model_2.keras')\n",
    "\n",
    "# 2. Rebuild your char‐level vocabulary/lookups from the training CSV\n",
    "df = pd.read_csv('work/kaggle_sentiment/tweet_sentiment_train.csv',\n",
    "                 encoding='utf-8', encoding_errors='replace')\n",
    "# concatenate all tweets into one long string\n",
    "text = df['text'].str.cat(sep='\\n')\n",
    "\n",
    "# get sorted list of all unique characters in the corpus\n",
    "vocab = sorted(set(text))\n",
    "vocab_size = len(vocab) + 1   # +1 for the OOV token that StringLookup will insert\n",
    "\n",
    "# make lookup layers exactly as in your notebook\n",
    "ids_from_chars = keras.layers.StringLookup(\n",
    "    vocabulary=vocab, mask_token=None  # no PAD token\n",
    ")\n",
    "chars_from_ids = keras.layers.StringLookup(\n",
    "    vocabulary=ids_from_chars.get_vocabulary(), \n",
    "    invert=True, \n",
    "    mask_token=None\n",
    ")\n",
    "\n",
    "# helper to turn a string into a vector of IDs\n",
    "def text_to_ids(s: str):\n",
    "    # unicode_split → a TF string Tensor of shape (len(s),)\n",
    "    chars = tf.strings.unicode_split([s], 'UTF-8')\n",
    "    return ids_from_chars(chars)\n",
    "\n",
    "# helper to turn a list of IDs back into text\n",
    "def ids_to_text(ids):\n",
    "    return tf.strings.reduce_join(chars_from_ids(ids), axis=-1).numpy().astype(str)\n",
    "\n",
    "# 3. Seed + pad to the fixed window length your model expects\n",
    "seed = \"Hey, \"\n",
    "seq_length = 100   # <— use whatever you trained with\n",
    "\n",
    "# convert seed to IDs and pad or trim to length\n",
    "seed_ids = text_to_ids(seed).numpy()[0]   # shape (len(seed),)\n",
    "seed_ids = seed_ids[-seq_length:]        # keep last seq_length chars\n",
    "seed_ids = np.expand_dims(seed_ids, 0)    # make batch of 1\n",
    "seed_ids = keras.preprocessing.sequence.pad_sequences(\n",
    "    seed_ids, maxlen=seq_length, padding='pre'\n",
    ")\n",
    "\n",
    "# 4. Generate one char at a time\n",
    "generated_ids = []\n",
    "num_chars = 512\n",
    "temperature = 1.0\n",
    "\n",
    "for _ in range(num_chars):\n",
    "    # predict next‐char distribution\n",
    "    preds = model.predict(seed_ids, verbose=0)[0, -1, :]  # (vocab_size,)\n",
    "    # apply temperature\n",
    "    preds = np.log(preds + 1e-8) / temperature\n",
    "    preds = np.exp(preds) / np.sum(np.exp(preds))\n",
    "    # sample\n",
    "    next_id = np.random.choice(len(preds), p=preds)\n",
    "    generated_ids.append(next_id)\n",
    "    # shift window and append\n",
    "    seed_ids = np.roll(seed_ids, -1, axis=1)\n",
    "    seed_ids[0, -1] = next_id\n",
    "\n",
    "# 5. Decode back to a string\n",
    "gen_text = ids_to_text(np.array([generated_ids]))\n",
    "print(seed)\n",
    "\n",
    "for text in gen_text:\n",
    "    print(text)\n",
    "\n",
    "del model\n",
    "K.clear_session()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30a4f2e3",
   "metadata": {},
   "source": [
    "## LSTM-Modell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e92f9f8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed: Hey, this is my LSTM: \n",
      "Generated continuation:\n",
      "\n",
      "one thing that should have been edited to take a load of two weeks, and then done!!\n",
      "LaLaLand... why am i liking that song so much, BE still seriously~\n",
      "?????? Oh, but my thumb hurts lol  cry excited here! Giviz you is for long and your unstant forget? laughter  its a whole day out low and play  http://plurk.com/p/rpa16\n",
      " We were in trouble, I did it make it through the first couple of days then I love and missed them\n",
      "   .: sorry you don`t forget that: http://www.stpecides.com/ - http://twitpic.com/66uj2 - hah\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.environ['TF_GPU_ALLOCATOR'] = 'cuda_malloc_async'\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gc\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras import backend as K\n",
    "\n",
    "\n",
    "# 1. Load your trained LSTM model\n",
    "model = keras.models.load_model('work/models/lstm_model.keras')\n",
    "\n",
    "# 2. Rebuild your char‐level vocabulary/lookups exactly as in training\n",
    "df = pd.read_csv('work/kaggle_sentiment/tweet_sentiment_train.csv',\n",
    "                 encoding='utf-8', encoding_errors='replace')\n",
    "text = df['text'].str.cat(sep='\\n')\n",
    "vocab = sorted(set(text))\n",
    "vocab_size = len(vocab) + 1   # +1 for any OOV token\n",
    "\n",
    "ids_from_chars = keras.layers.StringLookup(\n",
    "    vocabulary=vocab, mask_token=None\n",
    ")\n",
    "chars_from_ids = keras.layers.StringLookup(\n",
    "    vocabulary=ids_from_chars.get_vocabulary(),\n",
    "    invert=True,\n",
    "    mask_token=None\n",
    ")\n",
    "\n",
    "def text_to_ids(s: str):\n",
    "    chars = tf.strings.unicode_split([s], 'UTF-8')\n",
    "    return ids_from_chars(chars)\n",
    "\n",
    "def ids_to_text(ids):\n",
    "    return tf.strings.reduce_join(chars_from_ids(ids), axis=-1).numpy().astype(str)\n",
    "\n",
    "# 3. Prepare your seed and pad/trim to the sequence length you trained with\n",
    "seed = \"Hey, this is my LSTM: \"\n",
    "seq_length = 100   # ← must match the seq length you used during training\n",
    "\n",
    "seed_ids = text_to_ids(seed).numpy()[0]      # shape (len(seed),)\n",
    "seed_ids = seed_ids[-seq_length:]           # keep the last seq_length tokens\n",
    "seed_ids = np.expand_dims(seed_ids, 0)       # batch size 1\n",
    "seed_ids = keras.preprocessing.sequence.pad_sequences(\n",
    "    seed_ids, maxlen=seq_length, padding='pre'\n",
    ")\n",
    "\n",
    "# 4. Sampling loop\n",
    "generated_ids = []\n",
    "num_chars = 512\n",
    "temperature = 1.0\n",
    "\n",
    "for _ in range(num_chars):\n",
    "    # model.predict will return logits over the vocab\n",
    "    logits = model.predict(seed_ids, verbose=0)[0, -1, :]  # shape (vocab_size,)\n",
    "    # apply temperature\n",
    "    logits = np.log(logits + 1e-8) / temperature\n",
    "    probs = np.exp(logits) / np.sum(np.exp(logits))\n",
    "    # draw one character ID\n",
    "    next_id = np.random.choice(len(probs), p=probs)\n",
    "    generated_ids.append(next_id)\n",
    "    # slide the window one step and append the new ID\n",
    "    seed_ids = np.roll(seed_ids, -1, axis=1)\n",
    "    seed_ids[0, -1] = next_id\n",
    "\n",
    "# 5. Decode and print\n",
    "gen_text = ids_to_text(np.array([generated_ids]))\n",
    "print(\"Seed:\", seed)\n",
    "print(\"Generated continuation:\\n\")\n",
    "for t in gen_text:\n",
    "    print(t)\n",
    "\n",
    "del model\n",
    "K.clear_session()\n",
    "gc.collect()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
